# -*- coding: utf-8 -*-
"""Python Scraper

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a06ozFHKErx8gQaeps2tcEnCoYaEPlwb
"""

# Install all required dependencies
# !pip install beautifulsoup4 requests readability-lxml lxml

#!/usr/bin/env python3
"""
Article scraping logic converted from TypeScript to Python.
Uses requests, BeautifulSoup, and readability-lxml for article extraction.

Install dependencies:
pip install requests beautifulsoup4 readability-lxml lxml
"""

import requests
from bs4 import BeautifulSoup
from readability import Document
from urllib.parse import urljoin, urlparse
from datetime import datetime
from typing import Optional, List, Dict, Any
import json


class ArticleScraper:
    """Scrapes article content, metadata, and images from a given URL."""

    def __init__(self, url: str, timeout: int = 15):
        self.url = url
        self.timeout = timeout
        self.headers = {
            'User-Agent': 'SmartStorySuiteBot/1.0 (+https://your-domain.com/bot-info)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }

    def fetch_article(self) -> Dict[str, Any]:
        """
        Fetches and extracts article content, metadata, and images.

        Returns:
            Dictionary containing:
                - article_text: Main article content
                - title: Article title
                - source: Source/site name
                - author: Author name (if available)
                - date: Publication date (if available)
                - primary_image: Primary og:image URL
                - additional_images: List of additional image URLs from content
                - html: Raw HTML content
        """
        try:
            # Fetch the HTML
            response = requests.get(self.url, headers=self.headers, timeout=self.timeout)
            response.raise_for_status()

            # Check content type
            content_type = response.headers.get('content-type', '')
            if 'text/html' not in content_type:
                print(f"Warning: Content type is not HTML ({content_type}). Attempting parse anyway.")

            html = response.text
            soup = BeautifulSoup(html, 'html.parser')

            # Extract metadata
            metadata = self._extract_metadata(soup)

            # Extract article content using Readability
            doc = Document(html)
            article_html = doc.summary()
            article_text = BeautifulSoup(article_html, 'html.parser').get_text()
            article_title = doc.title()

            # Fallback if Readability fails
            if not article_text or len(article_text.strip()) < 150:
                print("Readability failed or content too short. Falling back to body text.")
                body_text = soup.body.get_text() if soup.body else ""
                if body_text and len(body_text.strip()) > 150:
                    article_text = body_text
                    article_title = soup.title.string if soup.title else "Title not found"
                else:
                    raise ValueError("Could not extract sufficient article content.")

            # Extract additional images from article content
            additional_images = self._extract_content_images(article_html, metadata['primary_image'])

            # Use metadata author if Readability didn't find one
            author = metadata['author']

            return {
                'article_text': article_text.strip(),
                'title': article_title or soup.title.string or "Title not found",
                'source': self._infer_source(),
                'author': author,
                'date': metadata['date'],
                'primary_image': metadata['primary_image'],
                'additional_images': additional_images,
                'html': html
            }

        except requests.exceptions.Timeout:
            raise Exception("Failed to fetch article: The request timed out.")
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 403:
                raise Exception("Failed to fetch article: Access denied (403). The site may block automated requests.")
            elif e.response.status_code == 404:
                raise Exception("Failed to fetch article: Not Found (404). Check the URL.")
            else:
                raise Exception(f"Failed to fetch article: {e.response.status_code} {e.response.reason}")
        except Exception as e:
            raise Exception(f"Error fetching or parsing article: {str(e)}")

    def _extract_metadata(self, soup: BeautifulSoup) -> Dict[str, Optional[str]]:
        """Extracts metadata from HTML meta tags."""
        metadata = {
            'primary_image': None,
            'date': None,
            'author': None
        }

        # Extract primary image (og:image)
        try:
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                potential_url = og_image.get('content')
                absolute_url = urljoin(self.url, potential_url)
                parsed = urlparse(absolute_url)
                if parsed.scheme in ['http', 'https']:
                    metadata['primary_image'] = absolute_url
                    print(f"DEBUG: Validated/Resolved primary image URL: {absolute_url}")
                else:
                    print(f"DEBUG: Resolved URL has non-http(s) protocol ({parsed.scheme}). Discarding.")
            else:
                print(f"DEBUG: No og:image meta tag found for {self.url}.")
        except Exception as e:
            print(f"DEBUG: Error extracting og:image: {str(e)}")

        # Extract date
        try:
            date_tag = (
                soup.find('meta', property='article:published_time') or
                soup.find('meta', attrs={'name': 'date'}) or
                soup.find('meta', attrs={'name': 'pubdate'}) or
                soup.find('meta', attrs={'name': 'timestamp'}) or
                soup.find('time', attrs={'datetime': True})
            )

            if date_tag:
                date_str = date_tag.get('content') or date_tag.get('datetime')
                if date_str:
                    try:
                        parsed_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                        metadata['date'] = parsed_date.strftime('%B %d, %Y')
                        print(f"DEBUG: Formatted scraped date: {metadata['date']}")
                    except Exception as date_error:
                        print(f"DEBUG: Could not parse date '{date_str}': {date_error}")
                        metadata['date'] = date_str
        except Exception as e:
            print(f"DEBUG: Error extracting date: {str(e)}")

        # Extract author
        try:
            author_tag = (
                soup.find('meta', attrs={'name': 'author'}) or
                soup.find('meta', property='article:author') or
                soup.find('meta', attrs={'name': 'article:author'}) or
                soup.find('meta', property='book:author')
            )

            if author_tag and author_tag.get('content'):
                author = author_tag.get('content').strip()
                if author.lower().startswith('by '):
                    author = author[3:].strip()
                metadata['author'] = author
                print(f"DEBUG: Scraped author from meta tag: {author}")
            else:
                print(f"DEBUG: No standard author meta tag found for {self.url}.")
        except Exception as e:
            print(f"DEBUG: Error extracting author: {str(e)}")

        return metadata

    def _extract_content_images(self, article_html: str, primary_image: Optional[str]) -> List[str]:
        """Extracts additional images from article content."""
        additional_images = []
        seen_urls = set()

        if primary_image:
            seen_urls.add(primary_image)

        try:
            content_soup = BeautifulSoup(article_html, 'html.parser')
            images = content_soup.find_all('img')

            print(f"DEBUG: Found {len(images)} <img> tags within Readability content.")

            MIN_DIMENSION = 50
            MAX_ADDITIONAL_IMAGES = 10

            for img in images:
                src = img.get('src')
                width = int(img.get('width', 0) or 0)
                height = int(img.get('height', 0) or 0)

                if src:
                    try:
                        absolute_src = urljoin(self.url, src)
                        parsed = urlparse(absolute_src)
                        is_likely_content = (width == 0 and height == 0) or width >= MIN_DIMENSION or height >= MIN_DIMENSION

                        if parsed.scheme in ['http', 'https'] and absolute_src not in seen_urls and is_likely_content:
                            additional_images.append(absolute_src)
                            seen_urls.add(absolute_src)

                            if len(additional_images) >= MAX_ADDITIONAL_IMAGES:
                                break
                    except Exception as url_error:
                        print(f"DEBUG: Could not parse or resolve image src '{src}': {url_error}")

            print(f"DEBUG: Added {len(additional_images)} valid additional image URLs.")

        except Exception as e:
            print(f"DEBUG: Error parsing article content HTML: {str(e)}")

        return additional_images

    def _infer_source(self) -> str:
        """Infers the source/site name from the URL."""
        parsed = urlparse(self.url)
        return parsed.hostname or "Unknown Source"


def main():
    """Example usage of the ArticleScraper (Colab-friendly)."""

    print("üì∞ Article Scraper")
    print("=" * 80)

    # ‚úÖ Prompt user for the article URL
    url = input("Enter the article URL: ").strip()

    if not url:
        print("Error: No URL provided.")
        return

    try:
        scraper = ArticleScraper(url)
        result = scraper.fetch_article()

        # Print results
        print("\n" + "=" * 80)
        print("SCRAPING RESULTS")
        print("=" * 80)
        print(f"\nTitle: {result['title']}")
        print(f"Source: {result['source']}")
        print(f"Author: {result['author'] or 'N/A'}")
        print(f"Date: {result['date'] or 'N/A'}")
        print(f"Primary Image: {result['primary_image'] or 'N/A'}")
        print(f"Additional Images: {len(result['additional_images'])}")
        print(f"\nArticle Text Length: {len(result['article_text'])} characters")
        print(f"\nFirst 500 characters:\n{result['article_text'][:500]}...")

        # Save to JSON
        output_file = "scraped_article.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump({
                "title": result["title"],
                "source": result["source"],
                "author": result["author"],
                "date": result["date"],
                "primary_image": result["primary_image"],
                "additional_images": result["additional_images"],
                "article_text": result["article_text"]
            }, f, indent=2, ensure_ascii=False)

        print(f"\n‚úÖ Results saved to {output_file}")

    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")


if __name__ == "__main__":
    main()

